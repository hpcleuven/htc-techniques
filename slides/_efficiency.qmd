# Efficiency


## Implicit parallelism

Some R functions/operations are parallel under the hood

::: {.fragment}
```r
A <- matrix(rnorm(25), nrow=5)
result <- A
for (i in 1:(power-1)) {
    result <- result %*% A
}
```
:::

::: {.fragment}
How do you know?

- Check the documentation
- Measure it
:::


## Benchmark environment

Experiments done on Sapphire Rapids node

- 2 Intel Xeon Platinum 8468 48-core
- 256 GB RAM

Module R/4.4.0-gfbf-2023a


## Parallel or not?

Time the execution of R script

```bash
$ time Rscript dgemm.R &> /dev/null

real    0m4.791s              # <1>
user    1m21.693s             # <2>
sys     0m31.345s
```

1. Real time, i.e., walltime
2. User time, i.e., CPU time used

82 seconds CPU time in 4.8 seconds walltime... [parallelism!]{.fragment}

::: {.fragment}
`%*%` use Basic Linear Algebra Subroutines (BLAS)

Uses multiple threads, each on a core
:::


## How to exploit that?

Easy: set `--cpus-per-task` > 1

::::: {.columns}
:::: {.column width=50%}
::: {style="font-size: 70%;"}
| CPUs per task ($n$) | walltime ($T_n$) |
|---------------------|------------------|
| 1                   | 109.6            |
| 2                   |  64.8            |
| 4                   |  50.5            |
| 8                   |  40.2            |
| 12                  |  34.3            |
| 24                  |  31.5            |
| 48                  |  31.0            |
| 96                  |  33.0            |
:::
::::
:::: {.column width=50% .fragment}
![](/img/walltime_vs_cpus_per_task.png)
::::
:::::

[Nice!]{.fragment}

::: notes
Results from dgemm/slurm-multicore_benchmark.out
:::

## Speedup

$S(n) = \frac{T_1}{T_n}$

::::: {.columns}
:::: {.column width=50%}
::: {style="font-size: 80%;"}
| $n$ | $T_n$ | $S(n)$ |
|-----|-------|--------|
| 1   | 109.6 | 1.00   |
| 2   |  64.8 | 1.69   |
| 4   |  50.5 | 2.17   |
| 8   |  40.2 | 2.73   |
| 12  |  34.3 | 3.20   |
| 24  |  31.5 | 3.48   |
| 48  |  31.0 | 3.54   |
| 96  |  33.0 | 3.32   |
:::
::::
:::: {.column width=50% .fragment}
![](/img/speedup_vs_cpus_per_task.png)
::::
:::::

[Nice? Maybe...]{.fragment}

::: notes
Results from dgemm/slurm-multicore_benchmark.out
:::


## Parallel efficiency

$E(n) = \frac{T_1}{n \cdot T_n}$

::::: {.columns}
:::: {.column width=50%}
::: {style="font-size: 80%;"}
| $n$ | $T_n$ | $S(n)$ | $E(n)$ |
|-----|-------|--------|--------|
| 1   | 109.6 | 1.00   | 1.00   |
| 2   |  64.8 | 1.69   | 0.85   |
| 4   |  50.5 | 2.17   | 0.54   |
| 8   |  40.2 | 2.73   | 0.34   |
| 12  |  34.3 | 3.20   | 0.27   |
| 24  |  31.5 | 3.48   | 0.14   |
| 48  |  31.0 | 3.54   | 0.07   |
| 96  |  33.0 | 3.32   | 0.03   |
:::
::::
:::: {.column width=50% .fragment}
![](/img/efficiency_vs_cpus_per_task.png)
::::
:::::

[Nice?, not so much...]{.fragment} [HPC infrastructure is *very* expensive!]{.fragment}

::: notes
Results from dgemm/slurm-multicore_benchmark.out
:::


## Lots of matrices...

Often same script run for many parameter settings ($N$)

::::: {.columns}
:::: {.column width=50%}
::: {style="font-size: 75%;"}
| $N$ | $T_N$    | $S(N)$  | $E(N)$ |
|-----|----------|---------|--------|
| 1   | 14.0     | 1.00    | 1.00   |
| 2   | 33.6     | 0.42    | 0.21   |
| 4   | 90.1     | 0.16    | 0.04   |
| 8   | 184.4    | 0.08    | 0.01   |
| 12  | 252.5    | 0.06    | 0.005  |
| 18  | 635.2    | 0.02    | 0.001  |
:::
::::
:::: {.column width=50% .fragment}
Oops!?!  Worse than serial?

::: {.fragment}
All R instances use 96 threads, so for $N = 18$...
[$18 \times 96 = 1728$ threads $\gg$ number
of cores]{.fragment}
:::

::: {.callout-warning .fragment}
Massive oversubscription
:::
::::
:::::

::: notes
Results from dgemm/slurm-multicore_omp_trouble.out
:::


## Control `OMP_NUM_THREADS`!

Set `OMP_NUM_THREADS=2`

::::: {.columns}
:::: {.column width=50%}
::: {style="font-size: 75%;"}
| $N$ | $T_N$ | $S(N)$ | $E(N)$ |
|-----|-------|--------|--------|
| 1   | 42.6  | 1.00   | 1.00   
| 4   | 43.7  | 3.90   | 0.98   |
| 8   | 49.6  | 6.87   | 0.86   |
| 12  | 65.3  | 7.82   | 0.65   |
| 24  | 74.5  | 13.7   | 0.57   |
| 36  | 85.2  | 18.0   | 0.50   |
| 48  | 94.3  | 21.6   | 0.45   |
:::
::::

:::: {.column width=50% .fragment}
$N = 8$ seems optimal, but what the heck, you're ruining the node anyway

Better go all the way to $N = 48$

::: {.callout-tip .fragment}
`OMP_NUM_THREADS` $\le$ number of cores
:::
::::
:::::

::: notes
Results from dgemm/slurm-multicore_gnu_parallel.out
:::


## The good news is...

Applications can be

- Memory bound ðŸ˜’
- CPU bound ðŸ˜Š
- I/O bound ðŸ˜—

::: {.callout-warning .fragment}
Benchmark you code!
:::


## Why performance degradation?

It's the memory architecture, stupid!

::: {style="font-size: 80%;"}
- 48 core Intel Sapphire Rapids CPU
  - 256 GB RAM, 8 memory channels at 4800 MHz
  - L3 cache per socket: 105 MB (shared, 2.2 MB per core)
  - L2 cache per CPU (core): 2 MB
  - L1 cache per CPU (core): 48 kB data
- 36 core Intel Icelake
  - 256 GB RAM, 8 memory channels at 3200 MHz
  - L3 cache per socket: 54 MB (shared, 1.5 MB per core)
  - L2 cache per CPU (core): 1.25 MB
  - L1 cache per CPU (core): 48 kB data
:::


## How to time?

- In job script: `time`
- In job script: `hyperfine`
  - Runs command multiple times
  - Reports mean, median, min, max, etc.
  - Can compare multiple commands
- For completed job
  ```bash
  $ sacct --cluster=<cluster> --format=JobID,TotalCPU,Elapsed --jobs=<job-id>
  ```


## And another thing...

Reading/writing many small files: [bad idea!]{.fragment}

::: {.callout-warning .fragment}
File systems for HPC optimized for large read/write operations, *not* for
many metadata operations!
:::

::: {.fragment}
Use appropriate file types, e.g.,

- HDF5
- Parquet
:::
