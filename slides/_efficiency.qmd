# Efficiency


## Representative Case

- Define a representative case
  (domain size, num. particles, protein size, genome size, etc)
- Runtime of +/- minutes
  (reduced timesteps)


## Which Resources Do I Need?

- Serial vs parallel code
  if parallel, MPI? OpenMP? or hybrid?
- CPU or GPU needed (read user guide)
- max memory needed (use [monitor](https://docs.vscentrum.be/compute/jobs/monitoring_memory_and_cpu_usage_of_programs.html) tool)
- I/O pattern and storage request


## Example: DGEMM

::: {.panel-tabset}

### About

- `dgemm.R`: matrix-matrix multiplication in `R`
- Using BLAS with OpenMP multi-threading: `%*%`
- Implicit parallelism
- Avoid oversubscription by setting `OMP_NUM_THREADS`
- Using parallel `foreach` for multiple matrices

### Jobscript

```{bash}
module load R/4.4.2-gfbf-2024a
module load R-bundle-CRAN/2024.11-foss-2024a

# call signature
export OMP_NUM_THREADS=1
dgemm.R --nr-matrices <M> --matrix-size <S> \
        --power <P> --seed <Z> --nr-cores <C>
```

- experiment with input variables `M, S, P, C`

### `R` script

```{r}
# Function to compute the matrix power
matrix_power <- function(A, power) {
    result <- A
    for (i in 1:(power-1)) {
        result <- result %*% A
    }
    result
}
```

:::


## Benchmark Environment

Experiments done on a wICE Sapphire Rapids node

- 2 Intel Xeon Platinum 8468 sockets, 48-cores each
- 256 GB RAM
- `module load R/4.4.2-gfbf-2024a`
- `module load R-bundle-CRAN/2024.11-foss-2024a`


## Parallel or not?

You don't know unless you:

::: {.fragment}
- Check the documentation
- Measure it
:::

::: {.fragment}

```bash
$ time Rscript dgemm.R --nr_matrices=1 --size 3333 --power 60 &> /dev/null

real    0m6.140s
user    3m58.968s
sys     1m26.297s
```

1. `real` gives the walltime
2. `user` gives the CPU time used
:::

::: {.fragment}
4 minutes CPU time in 6 seconds walltime ... [parallelism!]{.fragment}
:::

::: {.fragment}
- `%*%` uses Basic Linear Algebra Subroutines (BLAS)
- Uses multiple threads, each on a core
:::


## Setting `--cpus-per-task > 1` {fontsize=80%}

::: {.panel-tabset}

#### Results

:::: {.columns}
::::: {.column width=50%}

<div style="font-size: 0.75em;">

| CPUs per task ($n$) | walltime ($T_n$) |
|---------------------|------------------|
| 1                   | 44.1             |
| 2                   | 24.0             |
| 4                   | 14.0             |
| 8                   | 9.1              |
| 12                  | 7.1              |
| 16                  | 6.2              |

</div>

:::::

:::: {.column width=50%}

<div style="font-size: 0.75em;">

| CPUs per task ($n$) | walltime ($T_n$) |
|---------------------|------------------|
| 24                  | 6.4              |
| 32                  | 6.5              |
| 48                  | 6.1              |
| 64                  | 5.7              |
| 96                  | 6.0              |

</div>

:::::

::::

#### Runtime

![](img/runtime_cpus_per_task.png)

#### Speedup

::: {.columns}
:::: {.column width=15%}
$S_n=\frac{T_1}{T_n}$
::::
:::: {.column width=85%}
![](img/speedup_cpus_per_task.png)
::::
:::

#### Efficiency

::: {.columns}
:::: {.column width=15%}
$E_n=\frac{S_n}{n}$
::::
:::: {.column width=85%}
![](img/efficiency_cpus_per_task.png)
::::
:::

#### Conclusion

:::: {.fragment}
- Poor parallel efficiency if `--cpus-per-task>2`
- HPC infrastructure is *very* expensive
- Find an efficient setup
::::

:::


## The good news is...

Applications can be

- Memory bound ðŸ˜’
- CPU bound ðŸ˜Š
- I/O bound ðŸ˜—

::: {.callout-warning .fragment}
Benchmarking reveals it!
:::


## Why performance degradation?

It's the memory architecture, stupid!

::: {style="font-size: 80%;"}
- 48 core Intel Sapphire Rapids CPU
  - 256 GB RAM, 8 memory channels at 4800 MHz
  - L3 cache per socket: 105 MB (shared, 2.2 MB per core)
  - L2 cache per CPU (core): 2 MB
  - L1 cache per CPU (core): 48 kB data
- 36 core Intel Icelake
  - 256 GB RAM, 8 memory channels at 3200 MHz
  - L3 cache per socket: 54 MB (shared, 1.5 MB per core)
  - L2 cache per CPU (core): 1.25 MB
  - L1 cache per CPU (core): 48 kB data
:::


## How to time?

- In job script: `time`
- In job script: `hyperfine`
  - Runs command multiple times
  - Reports mean, median, min, max, etc.
  - Can compare multiple commands
- For completed job (including overheads)
  ```bash
  $ sacct --cluster=<cluster> --format=JobID,TotalCPU,Elapsed --jobs=<job-id>
  ```


## And another thing...

Reading/writing many small files: [bad idea!]{.fragment}

::: {.callout-warning .fragment}
File systems for HPC optimized for large read/write operations, *not* for
many metadata operations!
:::

::: {.fragment}
Use appropriate file types, e.g.,

- HDF5
- Parquet
:::

::: {.callout-tip .fragment}
Use `$VSC_SCRATCH_NODE` for SSD on the node.
:::