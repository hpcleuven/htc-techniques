# Efficiency


## Test Environment

- Define a representative case
  (domain size, num. particles, protein size, genome size, etc)
- Runtime of +/- minutes
  (reduced timesteps)
- Use modules, if available


## Which Resources Do I Need?

- Serial vs parallel code;
  if parallel, MPI? OpenMP? or hybrid?
- CPU or GPU needed (read user guide)
- max memory needed (use [monitor](https://docs.vscentrum.be/compute/jobs/monitoring_memory_and_cpu_usage_of_programs.html) tool)
- storage demand, and I/O pattern and


## Example: DGEMM

::: {.panel-tabset}

### About

:::: {.incremental} 
::::: {style="font-size: 75%;"}

- `dgemm.R`: matrix-matrix multiplication in `R`
- Takes arguments $N$ and $p$
- Generates matrix $A \in \mathbb{R}^{N \times N}$, elements normally distributed
- Computes $A^p$
- Determines minimum and maximum diagonal element
- Using BLAS with OpenMP multi-threading: `%*%`
- Implicit parallelism
- Avoid oversubscription by setting `OMP_NUM_THREADS`
- Using parallel `foreach` for multiple matrices

:::::
::::

### Jobscript

```bash
module load R/4.4.2-gfbf-2024a
module load R-bundle-CRAN/2024.11-foss-2024a

# call signature
export OMP_NUM_THREADS=<T>
dgemm.R --nr-matrices 1 --nr-cores 1 --seed 1234 \
        --power <P> --matrix-size <S>
```

- Let `P=60`
- Experiment with input variables `T, S`

### `R` script

```r
# Function to compute the matrix power
matrix_power <- function(A, power) {
    result <- A
    for (i in 1:(power-1)) {
        result <- result %*% A
    }
    result
}
```
:::: {.fragment}
- `%*%` uses Basic Linear Algebra Subroutines (BLAS)
- Uses multiple threads, each on a core
::::

:::


## Benchmark Environment

Experiments done on a wICE Sapphire Rapids node

- Intel Xeon Platinum 8468, 2 sockets, 48-cores each
- 256 GB RAM
- `module load R/4.4.2-gfbf-2024a`
- `module load R-bundle-CRAN/2024.11-foss-2024a`


## The good, the Bad and the Ugly!

Applications can be

- CPU bound ðŸ˜Š
- Memory bound ðŸ˜’
- I/O bound ðŸ˜—

::: {.callout-tip .fragment}
Benchmarking reveals it!
:::


## How to time?

::: {.incremental}
- In job script: `time`
- In job script: `hyperfine`
  - Runs command multiple times
  - Reports mean, median, min, max, etc.
  - Can compare multiple commands
- For completed job (including overheads)
  ```bash
  $ sacct --cluster=<cluster> --format=JobID,TotalCPU,Elapsed --jobs=<job-id>
  # or
  $ slurm_jobinfo <JobID>
  ```
:::

## And another thing ...

Reading/writing many small files: [bad idea!]{.fragment}

::: {.callout-warning .fragment}
File systems for HPC optimized for large read/write operations, *not* for
many metadata operations!
:::

::: {.fragment}
Use appropriate file types, e.g.,

(a) HDF5 &emsp; b. Parquet &emsp; c. ADIOS2 &emsp; ...
:::

::: {.callout-tip .fragment}
Use `$VSC_SCRATCH_NODE` for the node SSD ($<$ 1TB on wICE).
:::


## Parallel or not?

You don't know unless you:

::: {.fragment}
- Check the documentation
- Measure it
:::

::: {.fragment}

```bash
$ time Rscript dgemm.R --nr_matrices=1 --size 3333 --power 60 &> /dev/null

real    0m6.140s
user    3m58.968s
sys     1m26.297s
```

1. `real` gives the walltime
2. `user` gives the CPU time used
:::

::: {.fragment}
4 minutes CPU time in 6 seconds walltime ... [parallelism!]{.fragment}
:::


## Setting `--cpus-per-task > 1` {fontsize=75%}

::: {.panel-tabset}

#### Results

```bash
srun --machine=wice --partition=batch_sapphirerapids \
     --nodes=1 --ntasks=1 --cpus-per-task=<n> ...
```

:::: {.columns .fragment}
::::: {.column width=50%}

<div style="font-size: 0.75em;">

| CPUs per task ($n$) | walltime ($T_n$) |
|---------------------|------------------|
| 1                   | 44.1             |
| 2                   | 24.0             |
| 4                   | 14.0             |
| 8                   | 9.1              |
| 12                  | 7.1              |
| 16                  | 6.2              |

</div>

:::::

:::: {.column width=50%}

<div style="font-size: 0.75em;">

| CPUs per task ($n$) | walltime ($T_n$) |
|---------------------|------------------|
| 24                  | 6.4              |
| 32                  | 6.5              |
| 48                  | 6.1              |
| 64                  | 5.7              |
| 96                  | 6.0              |

</div>

:::::

::::

#### Runtime

![](img/runtime_cpus_per_task.png)

#### Speedup

::: {.columns}
:::: {.column width=15%}
$S_n=\frac{T_1}{T_n}$,
$n\in[1, 96]$
::::
:::: {.column width=85%}
![](img/speedup_cpus_per_task.png)
::::
:::

#### Efficiency

::: {.columns}
:::: {.column width=15%}
$E_n=\frac{S_n}{n}$,
$n\in[1, 96]$
::::
:::: {.column width=85%}
![](img/efficiency_cpus_per_task.png)
::::
:::

:::
